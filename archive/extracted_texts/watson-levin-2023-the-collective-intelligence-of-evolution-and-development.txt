The collective intelligence of evolution and development

Abstract

Collective intelligence and individual intelligence are usually considered to be fundamentally different. Individual intelligence is uncontroversial. It occurs in organisms with special neural machinery, evolved by natural selection to enable cognitive and learning functions that serve the fitness benefit of the organism, and then trained through lifetime experience to maximize individual rewards. Whilst the mechanisms of individual intelligence are not fully understood, good models exist for many aspects of individual cognition and learning. Collective intelligence, in contrast, is a much more ambiguous idea. What exactly constitutes collective intelligence is often vague, and the mechanisms that might enable it are frequently domain-specific. These cannot be mechanisms selected specifically for the purpose of collective intelligence because collectives are not (except in special circumstances) evolutionary units, and it is not clear that collectives can learn the way individual intelligences do since they are not a singular locus of rewards and benefits. Here, we use examples from evolution and developmental morphogenesis to argue that these apparent distinctions are not as categorical as they appear. Breaking down such distinctions enables us to borrow from and expand existing models of individual cognition and learning as a framework for collective intelligence, in particular connectionist models familiar in the context of neural networks. We discuss how specific features of these models inform the necessary and sufficient conditions for collective intelligence, and identify current knowledge gaps as opportunities for future research.

Keywords

Evolution, machine learning, networks, individuality, neural networks

Introduction

The identification of a suitable theoretical framework and appropriate engineering principles for collective intelligence are open problems. In this paper, we begin to address these gaps by developing a synthesis of perspectives usually considered to be quite distinct. To do this, we first dissolve several limiting misconceptions that cause collective intelligence and individual intelligence to be treated as separate topics; second, we introduce a speculative conceptual framework to unify them.

For an intelligence to belong properly to a collective, it must arise not from the cleverness of its members but from having the right kind of functional relationships between them. What kinds of functional relationships, and in what specific organisation, are required to turn a collective that is not intelligent into a collective that is? We use a specific understanding of cognition and learning that is already well-developed for individual intelligence to synthesise collective intelligence with aspects of development and evolution.

In particular, we explore how connectionist models of cognition and learning, familiar in neural network models of individual intelligence, can address this question, and how they signpost directions for future research in collective intelligence. We especially emphasize the known emergent properties of cellular collectives as instructive examples of collective intelligence at a sub-organismal scale.

Individual and collective intelligence are distinct phenomena. Or are they?

At first glance, it might seem that models of individual intelligence are not relevant to collective intelligence. Individuals have brains that can cognise and learn, and although colonies and swarms might be composed of individuals with brains, the collective as a whole is not a brain and cannot cognise or learn. Moreover, it is easy to understand why the component parts of an individual work together so well because adaptive processes at the organismic level, such as evolution by natural selection and reward-based reinforcement, select or reward them for doing so. In contrast, collectives are composed of multiple evolutionary units or distributed multi-agent systems and thus present unique credit-assignment problems that complicate reinforcement of such adaptive processes. Such distinctions seem to justify the consideration of collective and organismic intelligences as different topics. We argue that these are false distinctions and there is a bigger, and much more interesting, picture. The basic tenets of this unified view are the following:

All individuals are collectives. All individuals are collectives, made of parts that used to be individuals themselves. This is true not only for multicellular organisms derived from unicellular ancestors but also for eukaryotic cells with multiple organelles arising from bacterial ancestors, and for simpler cells that contain the first chromosomes arising from the union of previously free-living self-replicating molecules (Godfrey-Smith, 2009; Maynard Smith and Szathmáry, 1997; Michod, 2000; Okasha, 2006; West et al., 2015). Moreover, the proper functioning of organisms – their robustness, adaptability and evolvability – depends on the continued autonomy of their component parts (Levin, 2019; 2021a). Multicellular organisms exhibit multi-scale autonomy, a dynamic interplay of competition and cooperation, and coordinated collective action inherent to their development, function and behaviour, while being a society of cells (Fields and Levin, 2022; Levin, 2019; 2022; 2023; Sonnenschein and Soto, 1999). Thus, individuals like you and I, and collectives like swarms and colonies, are not as categorically different as they first appear.

All intelligences are collectives. Individual intelligence, in the familiar guise of a central nervous system or a brain, arises from the interaction of many unintelligent components (neurons) arranged in the right organisation with the right connections. This is the foundation of connectionism; that intelligence resides not in the individual parts but in the arrangement of the connections between them (LeCun et al., 2015; Watson et al., 2016; Watson and Szathmáry, 2016). The individual neuron is not where all the interesting cognition and learning occur. It is the distributed collective activity in the network that constitutes cognition and changes to the organisation of network connections that constitute learning. So brains are collectives, thus collectives of the right kind do cognise and learn. In fact, brains provide the archetypal example of an intelligent collective.

Cognition and learning are substrate-independent. The principles of distributed cognition familiar in artificial neural networks can be implemented by any network of signals and non-linear responses to suitably weighted inputs (Evans et al., 2022; Stern and Murugan, 2022; Watson et al., 2016). Gene-regulation networks, ecological networks and social networks can all compute in the same sense as neural networks if the connections are suitably arranged (Biswas et al., 2021; Davies et al., 2011; Herrera-Delgado et al., 2018; Power et al., 2015; Szabó et al., 2012; Tareen and Kinney, 2020; Watson et al., 2014). In development and organismic biology, many different levels of adaptive networks exist aside from neural networks, including gene-regulation networks, protein networks, metabolic networks, morphogen diffusion networks and endocrine systems. In addition, it is clear that morphogenesis, physiological function and the adaptive processes of robustness and repair all require information integration and collective action that constitute cognition – in many cases without neurons. Each of these phenomena exhibits the same learning behaviours, including the storage and retrieval of multiple associative memories, effecting classification and recognition with generalisation capabilities, and learning to solve combinatorial optimisation problems better with experience (Watson et al., 2011a; Watson et al., 2011b, 2011c).

The credit assignment problems inherent in collective intelligence are fundamental in all cognition and learning, and in all biological individuality. It is true that collective intelligence is fundamentally about collectives - meaning that we cannot presuppose the system as a whole to be a single selective or utility-maximising unit. However, when we take a larger perspective – for example, one concerned with their emergence over developmental or evolutionary timescales – neither can we presuppose that apparently unambiguous individuals have always been (single) selective or utility-maximising units. Thus, the credit assignment issues of collective intelligence are not categorically distinct from related core issues in individual adaptation, evolution and intelligence.

Towards a unified theory of intelligence and cognition

In collectives, each component selects behaviours based on the rewards they receive for their own actions (Figure 1(a)). In intelligent systems, the reward feedback is effectively operating at a higher level – and the system as a whole selects behaviours based on the rewards received by the system as a whole (Figure 1(b)). Accordingly, it makes sense that the system selects behaviours that facilitate long-term collective reward. But operationally, each component within the intelligent system is still autonomous, selecting individual actions based on individual rewards given the relational context they find themselves in. The question is, what kinds of interaction structures cause collectives to behave like intelligent agents, exhibiting information integration and coordinated action that effect reward feedback at the system level? (Figure 1(c)). Here, we propose a formalism for thinking about these issues as a set of hypotheses to drive future research.

Establishing these commonalities has significant consequences for understanding: since some of these questions have well-developed answers in the context of individual intelligences, those answers can be transferred to provide a framework for approaching collective intelligences. While connectionist models of cognition and learning do not have all the answers, they do identify the kind of relationships that turn a collection of unintelligent components into a collective intelligence, with cognitive and learning abilities that belong to the whole and not the parts. Additionally, connectionist models identify conditions where collective intelligence can arise bottom-up, using only distributed learning mechanisms without system-level or global feedback.

Figure 1. Perspectives on individual intelligence and collective intelligence. Complex systems are composed of many interacting components. But where is the agent – at the component level or the system level? (a) Swarms are often characterised as collectives, but the agency (reward feedback and decision making) is generally attributed to each component. These are obviously collectives but not obviously intelligent. (b) Animal intelligence is often characterised as a single, system-level agent (exhibiting information integration and collective action), but the components are generally considered to be ‘parts’ without agency. These systems are obviously intelligent but not obviously collectives. (c) In reality, all intelligences are made out of components that act on local information based on individual feedbacks. In a multicellular organism, for example, the individual cells exhibit agency based on local information and rewards, and the system (cellular swarm) as a whole does also, exhibiting information integration and anatomical decision making at the system-scale.

Table 1. What kinds of relationships are necessary to turn a society into an individual? A central aspect of how intelligence arises from a collection of subunits is the specific communication and functional linkages between them, as well as the algorithms for updating those interactions in light of experience. It is thus essential to determine what kinds of architectures underlie different degrees of agency (which support memory, problem-solving, information integration and collective action, higher-level autonomy, etc.) across the continuum. Here, we leverage connectionist models of cognition and learning (top row) to specify known architectures that embody key waypoints along the collective intelligence spectrum (bottom row), as well as to identify knowledge gaps that highlight opportunity for next steps in this field. References indicate examples of potentially relevant models where available. Shading indicates speculative suggestions and opportunities for future research. In the final column, the biological examples are known but the relevant topology is not.

Watson and Levin

Table I. (continued)

| Interactions between multiple evolutionary units, or credit assignment at level of individual components | Independent evolution of multiple species within an ecological setting. Also, selection on independent genes under free recombination (combinations of alleles are not heritable). (Chastain et al., 2014) | Evolution of ecological interactions under individual selection limited to one trophic level? | Evolution of ecological interactions with multiple trophic levels? | Evolution of ecological interactions with arbitrary (non-trophic) species interactions (Power et al., 2015) | Evolutionary transitions in individuality? | Flexible individuality (e.g. slime molds), autonomous modules that goal-seek/ exhibit multi-scale competency (Fields & Levin, 2020; Gawne et al., 2020). |
|---|---|---|---|---|---|---|
|  | Ecological configurations could be linearly separable function of environmental conditions, thus ecosystem has evolved structure and can change strength, but not direction, of selection on other components | Ecological configurations could be non-linearly separable functions of environmental conditions (including other species), thus interactions can change the direction of selection on other components and the ecosystem responds in a non-decomposable manner. | Demonstrates ability to store and recall multiple 'ecological memories' at system level without system-level selection, and improve resource allocation problem-solving with experience (Power, 2019) | Associations between low-level evolutionary units become subsumed by higher-level units. Each layer integrates information and coordinates action of units at layer below. | Enables evolutionary search in higher-level components that autonomously fill in the details on-the-fly. |


1. Feed-forward mappings and recurrent dynamics: Artificial neural networks are often used to represent (and learn) a mapping between inputs and outputs (e.g. for classification or regression tasks). One of the simplest 'feed-forward' networks is the single-layer Perceptron where an output node fires if the sum of its weighted inputs exceeds a threshold (more generally the output is some non-linear monotonic function, e.g. a sigmoidal function, of the weighted sum of inputs). This is capable of representing simple input-output relationships and learning to classify inputs according to such relationships. In other cases, connections can be recurrent, that is, connections can form loops and thus states can be influenced by inputs from previous time steps and the system can continue to hold internal state after the input is removed. They can also, thereby, exhibit temporally extended dynamical behaviours. Accordingly, in recurrent networks we are often interested in the dynamical attractors of the system (which are a function of the system's own internal history not just current inputs) rather than instantaneous values of designated outputs or the input-output relationship. The Hopfield network is a simple example (Appendix Box 1). Because connections are symmetric (with no self-connections) in the Hopfield network, its dynamics have only fixed point attractors (‘memories'), but more general recurrent architectures may have periodic or chaotic dynamical behaviours.

2. Deep representations and non-linearly separable functions: The single-layer Perceptron has important limitations. Specifically, although it can represent 'linearly separable functions' where the response to a change in one input changes magnitude depending on the value of another input (i.e. the responses are not independent), it cannot represent non-linearly separable functions where the response to a change in one input changes direction depending on the value of another input (Watson et al., 2022). This type of interdependence is important because in the linearly separable case, if an input contributes positively to an output in one context, it never contributes negatively in another. This means the single-layer Perceptron can represent cases where 'working together' changes the benefit an individual input can receive (from doing what they were doing anyway), but it cannot represent cases in which working together requires an individual to do the opposite behaviour, move in the other direction or do something opposed to what they were doing when they worked alone or in some other context. Representing non-linearly separable functions requires a network with multiple layers - a multi-layer Perceptron (MLP). In principle, an MLP can represent any function of the inputs given sufficient 'hidden' variables (units that are neither inputs nor outputs but constitute an intermediate layer of representation). In practice, it is frequently useful to employ more layers (with fewer nodes each) because this affords a different inductive bias and generalisation. These are known as deep networks (LeCun et al., 2015).

3. Deep and recurrent networks: Whilst there are many other architectures used in artificial neural networks, two others are worth mentioning. A deep auto-encoder is a network that compresses a high-dimensional input space into a low-dimensional representation. A decoder decompresses the low-dimensional representation back into the original high-dimensional space. The compressed encoding can be interpreted as a low-dimensional model of the samples observed on the input space. Changes to the variables of the compressed representation produce large, coordinated changes to the variables in the input space. Lastly, the deep belief network (DBN) (Hinton et al., 2006) is quite a special type of network, and its architecture has particularly relevant properties. The DBN has a layered architecture that can be used to learn compressed representations like the auto-encoder, and within each layer the nodes have recurrent connections. This gives the DBN both the potential to represent low-dimensional recodings of the original input space and to have dynamical attractors that stably retain their state at that higher level of representation.

Implications for evolutionary intelligence and basal cognition

Naturally, for a collection of individuals to exhibit any kind of collective intelligence, it is, at the very least, necessary that the behaviour of one individual has some sensitivity to the behaviour of another. Such interactions can coordinate behaviours to take advantage of scenarios where the benefit/reward or fitness that one individual receives is sensitive to the behaviour of another. However, if this credit-assignment interaction (or fitness epistasis) constitutes a linearly separable function this is not really a difficult problem; although the benefit they receive will vary in different contexts, the behaviour that maximises their benefit is always the same. In contrast, when the credit that one individual receives has an interaction with the credit that another individual receives which constitutes a non-linearly separable function (Watson and Thies, 2019) (or reciprocal sign epistasis, (Weinreich et al., 2005)), this requires that one individual can change its behaviour (or 'do the opposite') depending on the context of what other individuals are doing. For a collective to coordinate behaviours to take advantage of such interactions, it must be able to represent non-linearly separable functions, which requires the interaction structure between individuals to have some depth (Watson et al., 2022).

These are just the kind of relationships that make the credit assignment or fitness of the whole not only different from the sum of the rewards/fitnesses of the parts but also a non-decomposable function. Intuitively, this changes our relationship from ‘how good this is for me depends on what you are doing' to 'what is best for me to do depends on what you are doing'. This is important because, when it is reciprocal, the fitness-affecting characteristics of one component only have meaning in the context of the other. In other words, it creates a 'we'; what we are doing, for example, whether our behaviours are coordinated or not, becomes a relevant variable (Watson et al., 2022; Watson and Thies, 2019).

Deep representations also have a special significance in recurrent networks. In non-hierarchical networks, the many connections between components can cause the system to hold state over time (i.e. internal states can be maintained as dynamical attractors even when the inputs to the network are removed or have changed). This enables the network to exhibit temporally extended behaviours, but it also has the effect that it becomes difficult to change the system state and, therefore, to be sensitive to system inputs. Getting out of one dynamical basin of attraction and into another can require large and/or specific state perturbations. The system acts as a whole but cannot ‘change its mind' easily (Hills et al., 2015; Nash et al., in prep; Watson et al., in review). This is problematic for organismic adaptability and evolutionary variability. In contrast, a hierarchical representation can cause coordinated behaviour in many downstream parts but retain the capacity for small changes to variables in the higher level representation to move all the downstream variables to a new state (Nash et al., in prep). A recent alternative model is provided by a network of neurons that have a 'decision cycle' that repeatedly re-decides which states to adopt with a timing based on learned connections (Watson et al., in review). By learning to synchronise the decision cycle of particular groups of components, this kind of network exhibits multi-scale problem-solving capabilities without having an explicit or pre-defined multi-layer structure.

Cascading control architectures – where a small number of variables cause large coordinated changes in the state of many downstream variables are common in organisms through many scales from molecular to morphological. This takes explanatory focus away from the collective and onto the units at deeper levels of the causal chain, for example, a gene cues the coordination of other biomolecules within the cell, and the germ line cues coordination of other cells within the organism. However, natural organisms are neither single-layer recurrent networks (with every component connected equally to every other like the Hopfield network) nor strictly feed-forward multi-level hierarchies (with components in one layer only connected to components in the layer below like the MLP). They are not quite like deep-belief networks either, of course, but they do contain elements of both cascading control and recurrent control architectures. This means that different levels of organisation can both be influenced by higher level control variables and be collectives that co-define and sustain their own (non-decomposable) meaning. These considerations suggest that this kind of deep and also partially recurrent architecture is relevant to the multi-scale autonomy observed in complex organisms.


Learning the structure of interactions

The previous section discussed how the types of relationships, and their organisation, might influence the type of information integration and coordinated action that could be exhibited by a collective. But how do such organisations arise? For this, we turn our attention from connectionist architectures to models of connectionist learning. A number of issues and observations are relevant to collective intelligence:


Gradient methods versus stochastic local search, supervised learning versus reinforcement learning. For many learning tasks, it is useful to express the error in the output (with respect to an input and a target) as a function of the connection strengths in the network. If this function is differentiable, then this can be used (in artificial machine learning methods) to define a gradient method which computes a change in the weights of the network that will systematically reduce the error. In biological evolution or emergent collective intelligence, there is no explicit target or desired output predetermined by an external agent or teacher. There is therefore no 'error' function, as such. The more relevant type of learning is reinforcement learning, where different outputs confer different rewards but the 'correct' output, or the pattern that maximises reward for a given input, is not used explicitly in training (and may be unknown). Natural selection can be used to increase the fit of an organism to its environment or improve rewards by adjusting weights in the same way. These basic observations are the basis of the formal equivalence between learning and evolution by natural selection (Campbell, 1956; Harper, 2009; Shalizi, 2009; Skinner, 1981; Watson and Szathmáry, 2016).

What makes learning systems smart, however, is not merely the ability to increase the fit of model parameters to data; what makes such systems interesting is that the parameters they adjust and the data to which they fit are not in the same space (Buckley et al., in prep; Watson and Szathmáry, 2016). For example, the quality of the network output is, in a direct sense, a function of the network outputs and how well these fit the environmental needs. But the parameters that are adjusted during learning are not these output variables per se. Rather they are the parameters of a model that produces these outputs – namely, the network of interactions connecting one node to another. This separation between 'model parameters' and 'solution space' is crucial because without it there is no possibility of using past experience to respond appropriately in novel situations, that is, generalisation (Watson and Szathmáry, 2016).


Generalisation is fundamental to learning and intelligence. Without it, a system can only respond to current inputs in a manner consistent with past rewards. At one extreme, if the future is going to be exactly like the past, this is fine. At the other extreme, if the future has nothing at all in common with the past, then there is not much that can be done about that. But, in other cases, the future is not the same as the past, but it shares some kind of underlying regularity in common with it. These are the cases where intelligence has some meaning. Specifically, a system that can generalise can act in a manner that is consistent with long-term rewards, even when this appears to oppose immediate or short-term interests. For individuals that interact with others in a collective, the ability to act in a manner that is consistent with long-term individual interest is frequently aligned with the ability to act in a manner that is consistent with collective interest (though it may be opposed by individual short-term interest). Although this ability might seem quite sophisticated and mysterious, connectionist models of cognition demonstrate that this does not require the parts to become more intelligent; only that the relationships between them are adjusted appropriately, which can be implemented by simple incremental gradient following (Appendix Box 1).


Unsupervised learning. It might seem curious that any kind of learning can occur without supervision or system-level reward feedback of some kind. How can a learning system know what to learn if nothing tells it what it is supposed to learn? Unsupervised learning builds a low-dimensional model of the input data. The changes to connections are not motivated by error minimisation or reward maximisation but purely by the fit of the model to the data. Hebbian learning ('neurons that fire together wire together' (Buckley et al., in prep; Watson and Szathmáry, 2016; Watson et al., 2014) (Appendix Box 1) reduces the effective degrees of freedom in the network dynamics in a manner that ‘mirrors' the degrees of freedom induced by past experience without being rewarded for that purpose or using an error function that targets it.



The level of credit assignment in reinforcement learning and collectives. Consideration of unsupervised learning has direct significance for the evolution and reward of collective intelligence. This is because reinforcement learning acting on the individual characteristics affecting their connections to others can result in dynamics that are equivalent to unsupervised learning at the system scale (Davies et al., 2011; Power et al., 2015; Watson et al., 2011a). Intuitively, if B is rewarded for being activated, then one of the ways it can increase its reward is to increase the strength of its connection from A (e.g. when A and the connection are positive). This increases the individual reward B receives right now, but it also makes the future activation of B correlated with the activation of A (the principle of Hebbian learning in another guise). The same considerations apply to A and its connection from B. Note that neither component is making the connection with the other because it is interested in the collective reward that A and B receive together, nor because it makes the future dynamics of the AB pair more consistent with their past correlation. Nonetheless, it does make the future dynamics of the AB pair more consistent with their past correlation (Watson et al., 2014).


This observation creates a fundamental linkage between the principles of individual learning or individual utility-maximisation and the principles of system-level or collective intelligence (Watson et al., 2011a). Note that the mechanism of Hebbian learning was identified by Donald Hebb to explain neural learning because it is the right way to modify synaptic connections if you want the network to model observed correlations. This equivalent mechanism, in contrast, is motivated bottom-up – it is a consequence of components that are incentivised only by short-term self-interest (given that they have connections with others that they can modify). In the same way that this distributed learning does not require system-level reinforcement, it also occurs in evolutionary systems without system-level selection (Power et al., 2015). Individual selection acting on members of an ecological community produces the same structural changes to connections (inter-specific interactions) simply because each is incentivised by selection to maximise its individual growth rate. This has the same consequences for the ecological assembly rules and succession dynamics as it does for the dynamical attractors of neural activations in the Hopfield network (Power et al., 2015).


How does distributed learning effect system-level rewards and credit assignment?. This distributed learning is not motivated by system-level rewards (total utility), nor does it involve system-level selection, but it has a systematic relationship to system-level rewards and fitness nonetheless. In multi-agent systems, the original dynamics, given a system of constrained interactions, are much like a ball rolling down hill – each individual decides how to act to maximise individual reward as determined by the constraints with others. This finds a local optimum in total utility, but only a local optimum. As the individuals modify their connections from others, the dynamics of the systems are channelled into trajectories that mirror the structure of past experience. If the system is subject to repeated shocks or perturbation, or experiences an episodic stress, causing it to visit a distribution of attractors over time, then what it 'learns' is a generalised model of the constraints it has experienced. Because this model is based on interactions between components and not on independent parameters, it is a correlation model that has the potential to generalise - responding in a way that resolves constraints between individuals better than any previous attractor visited (Buckley et al., in prep; Watson et al., 2011a). In this way, short-sighted self-interested agents form relationships with one another that sometimes cause them to make different decisions (given the new weightings of the options created by the new relationships). Also, these new choices better optimise the long-term collective interests of the system as a whole (Buckley et al., in prep; Watson, accepted; Watson et al., in review; Watson et al., 2011a).

This bottom-up incremental adjustment of relationships can thus increase system-level welfare. It does so in a manner that is functionally equivalent to distributed learning mechanisms familiar in artificial neural networks, without presupposing system-level rewards or credit assignment. Moreover, in so doing, it creates a non-decomposable whole (attractors that are non-linearly separable functions of the inputs and depend on the system's own internal history), which means that credit assignment or reward at the level of individual parts and their individual behaviours becomes ineffective. Instead, credit assignment (if it applies at all) and any possibility of effecting modified behaviours through reward become meaningful only at the higher level of organisation.

Modelling collective intelligence and basal cognition: Evolutionary individuality, organismic individuality and cognition are coextensive


As discussed above, the basic computational elements of such distributed learning are substrate-agnostic and common to a wide range of biological networks (Cervera et al., 2018; Pietak and Levin, 2016, 2017). However, the conditions for distributed learning are non-trivial; not all of these networks may meet them. The important thing to note is that there is no requirement for an incentive to model long-term or collective consequences of individual actions, or for a system-level incentive to model the structure or pattern of observations. We do not yet know which of these biological systems might meet these conditions and the extent to which this influences their collective intelligence. But it is known that organismic individuality evolved through a bottom-up process of collective intelligence, resulting in information integration and coordinated action so well-organised that we observe a new level of organismic and evolutionary individuality. The principles of connectionist cognition and learning described above provide a roadmap of gaps and opportunities that future research might explore to better understand how such emergent individuality occurs. In particular, the architecture of the interactions - whether they are feed-forward or recurrent, capable of representing non-linearly separable functions or not, shallow or deep or some mixture of these characteristics - has important consequences for the type of cognitive model they can represent.


The ecological models developed thus far demonstrate that connectionist learning principles are relevant to collective intelligence in systems that are not (yet) evolutionary units. They fall short, however, of demonstrating the spontaneous evolution of a new level of individuality. In algorithmic terms, such models cannot do the ‘chunking' of the search space or rescaling of the search process that is facilitated by the induction of deep models (Caldwell et al., 2018; Mills, 2010; Mills et al., 2014; Watson et al., 2011b; Watson et al., 2016; Watson et al., 2009). We hypothesise that this is because they are single-level networks of symmetric interactions; our roadmap supports the idea that the evolutionary transitions in individuality correspond to deep interaction structures (Czegel et al., 2019; Watson et al., 2022) or perhaps other mechanisms of multi-scale dynamics (Watson, accepted; Watson et al., in review).



We propose that some of the gaps in this picture might be addressed by exploring the hypothesis that evolutionary individuality, organismic individuality and cognition are coextensive (Watson et al., 2022). The idea is that acting in a manner consistent with long-term collective interests, in particular when this conflicts with short-term self-interest, is not just a hallmark of collective intelligence but is in fact what constitutes cognition and individuality at the collective level. This can perhaps be formalised through the consideration of non-linearly separable functions. Specifically, if a system of functional interactions among the parts represents a non-linearly separable function, then the incentive of the whole is related to the incentives of the parts only in a non-decomposable way (Watson et al., 2022).

Conclusions


Commonalities between cognitive and evolutionary processes and those that shape growth and form have been hinted at in the past (Grossberg, 1978; Pezzulo and Levin, 2015; Spemann, 1967). We argue that conceptual advances in the links between machine learning and evolution now provide quantitative formalisms with which to begin to develop testable models of collective intelligence across scales. From subcellular processes, to cellular swarms during morphogenesis, to ecological dynamics on evolutionary timescales – all of these processes are driven by the scaling of reward dynamics that bind subunits into collectives that better navigate novel problem spaces.


In addition to shedding light on biological evolution, a better understanding of the origin and operation of collective intelligences would have a number of practical applications. Molecular medicine today is focused almost entirely on the micro-hardware of life - modifying DNA and rewiring molecular pathways - with limited success due to difficult inverse problems (Lobo et al., 2014). The capacity to manipulate the collective intelligence of cell groups might offer powerful ways to guide native and synthetic morphogenesis top-down (Pezzulo and Levin, 2016). Insights gleaned from biological systems could also significantly enhance the engineering of intelligent robots whose behaviour results from cooperation, competition and merging of subunits across multiple levels of organization.

Harnessing the native capability of collective intelligence in the service of biomedicine or bioengineering will require a much better understanding of how to identify, characterise and motivate emergent agents in anatomical, physiological and transcriptional spaces (Levin, 2022a; Pezzulo and Levin, 2015). As a starting point, we need to develop appropriate formalisms for top-down control of multi-scale intelligent agents of diverse composition. We argue that the tools and concepts of machine learning, behavioural neuroscience and evolutionary biology apply to problems of collective intelligence at multiple scales and offer a promising way forward.

There is a deep, fundamental symmetry between the origin of new evolutionary individuals from competent subunits and the assembly of an integrated cognitive agent as a collective intelligence composed of sub-agents. Future experimental and in silico work will quantitatively identify the necessary and sufficient relationships that effect such transitions. Such work has the potential to drive a flourishing sub-field of collective intelligence with implications ranging from basic evolutionary biology to regenerative medicine and artificial intelligence.


Acknowledgements

We thank Chris Buckley, Frederick Nash, Jamie Caldwell, Christoph Thies and David Prosser for many useful discussions on these topics, and Julia Poirier for editorial assistance with the manuscript. M.L. acknowledges support via grant 62212 from the John Templeton Foundation, grant TWCF0606 of the Templeton World Charity Foundation, and Science Research 2.0. R.A.W. acknowledges support of the John Templeton Foundation, Grant 62230 (the opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the John Templeton Foundation).

Declaration of conflicting interests

The author(s) declared no potential conflicts of interest with respect to the research, authorship and/or publication of this article.

Funding

The author(s) disclosed receipt of the following financial support for the research, authorship and/or publication of this article: This study was supported by the John Templeton Foundation (62230), Templeton World Charity Foundation and Science Research 2.0.

ORCID ID

Michael Levin https://orcid.org/0000-0001-7292-8084


References

Abramson CI (1994) A Primer of Invertebrate Learning : The Behavioral Perspective. 1st edition. Washington D.C.: American Psychological Association.

Appendix

Box 1: The Hopfield network and collective behaviour

The Hopfield network is a neural network model (and a general model of dynamical systems in many domains from ferro-magnets to ecological communities) described by a set of nodes (either binary threshold units or sigmoidal continuous response units) connected to each other with symmetric connections (and no self-connections). Each node 'fires' if the weighted sum of inputs from other nodes is sufficiently strong. Given that some weights can be zero, this is a fairly general concept of a dynamical system described by a set of interactions between variables (Figure A1). The special qualification that the weights are symmetric (and no self-weights) is important, however, because it means that the dynamics can be described by the local minimisation of an energy function and the attractors of the dynamics are fixed points (i.e. state configurations where no units change value).

One interesting behaviour of this kind of network is the ability to store multiple patterns of activation in the connections of the network and generate, recognise or recall stored patterns through associative memory. A pattern (such as an image, or a set of features, describing a food type, habitat or a predator) can be stored by setting the units to match the (signed) pattern values and then applying Hebbian learning to the weights such that a change in the connection between two neurons is proportional to the product of the state values (a.k.a. neurons that fire together wire together). This kind of change to a connection makes it easier (lower energy) for the two states it connects to fire together in future. For example, if both states are firing at the same time, the connection strength is increased, meaning that activation in one stimulates activation in the other, making it more likely that they both activate together in future. This has the effect of lowering the energy of this configuration, drawing the network state towards this pattern in future – that is, forming a memory of the pattern. One network can store multiple patterns simultaneously, and stored patterns can be generated from the network (from the set of patterns that it has stored) by initialising the state values at random and running the network to an attractor. A given pattern can also be recalled or recognised by presenting a partial or noisy input - causing the network to complete or recreate the entire/uncorrupted pattern that was closest to this stimulus (i.e. a content-addressable memory (Hopfield, 1982)). Over a set of patterns stored in this way, connections in the network model the correlations (commonly occurring combinations) of state values. This ‘associative model' of past state configurations can also generalise for example, generate a pattern that has the same underlying structural relationships as those observed during learning but is nonetheless novel, that is, different from any specific pattern observed during training.

These networks can also exhibit problem-solving behaviour. If the connections of the network correspond to the constraints of a problem (i.e. the agreement or disagreement of two variables confers a change in solution quality proportional to the magnitude of the weight between them), then the natural dynamics of the state variables is to change in a manner that decreases violated constraints, causing the network to discover locally optimal solutions to the problem (Hopfield and Tank, 1986). Moreover, under certain conditions, the addition of relatively slow Hebbian learning to the weights, applied whilst the state variables visit a distribution of such locally optimal solutions, causes the network to form an associative memory of its own behaviour (a 'self-modelling dynamical system' (Buckley et al., in prep; Watson et al., 2011a), or a memory of the locally optimal solutions it visits. Because this associative memory can generalise, it can change its own dynamics in a manner that improves the ability of the network to resolve problem constraints, and with positive feedback, it can thus learn to discover high-quality solutions more reliably over time (reinforcement) and also find solutions that are better than any solutions found before the application of such learning (i.e. true optimisation).

Note that the memory, recall/recognition and problem-solving behaviour of the network, and the learning mechanisms that organise the connections to achieve this, are fully distributed and decentralised. During recall, each neuron fires if its inputs are strong enough, without centralised control. And during learning, the update to each connection is proportional to the product of activation in the two neurons it connects, without reference to any global feedback, performance measure or testing of consequences from this change. Crucially, these recognition/recall and problem-solving behaviours can be exhibited by the network as a whole but cannot be exhibited by the individual components therein (nor explained by any average or sum of their individual behaviours). Neither do these new system-level behaviours result from changes to the behaviours of individual units but only from a change to the organisation of connections between them. These observations are important for collective intelligence for the following reasons. Where individuals have behaviours that are sensitive to the behaviours of others, adjustments are made to the organization of these relationships, either in terms of their selective strength (rHN-s, Watson et al., 2011a), their generation of variability (rHN-g, Watson et al., 2011c), or their timing (rHN-t, Watson et al., in prep). Such adjustments, which are made using only local information, are sufficient to produce non-trivial collective behaviours (collective memory, recognition, learning, generalization and problem solving) without centralized control or global feedback on performance.


Figure A1. Hopfield network architecture.