CellPress
Opinion
Trends in
Genetics
What does evolution make? Learning in living
lineages and machines
Benedikt Hartl 1,2,4 and Michael Levin 1,3,4,*

How does genomic information unfold, to give rise to self-constructing living organisms with problem-solving capacities at all levels of organization? We review recent progress that unifies work in developmental genetics and machine learning (ML) to understand mapping of genes to traits. We emphasize the deep symmetries between evolution and learning, which cast the genome as instantiating a generative model. The layer of physiological computations between genotype and phenotype provides a powerful degree of plasticity and robustness, not merely complexity and indirect mapping, which strongly impacts individual and evolutionary-scale dynamics. Ideas from ML and neuroscience now provide a versatile, quantitative formalism for understanding what evolution learns and how developmental and regenerative morphogenesis interpret the deep lessons of the past to solve new problems. This emerging understanding of the informational architecture of living material is poised to impact not only genetics and evolutionary developmental biology but also regenerative medicine and synthetic morphoengineering.

Whence the endless forms most beautiful?

Living forms present three fundamental challenges to our understanding: first, they self-assemble - performing all of the decision-making needed to construct a functional, complex body while the computational material itself is being reorganized on-the-fly. Second, they reach the correct target morphology reliably, utilizing heredity mechanisms to propagate specific patterns of form and behavior through time. Crucially, third, this process is almost never hard-wired, but instead offers immense plasticity, able to complete morphogenetic tasks despite perturbations of external environment and internal components [1]. This capacity to navigate the morphospace of possible anatomies, to produce the correct final pattern in the face of novel situations, or to create something completely different (never before seen by evolution) but nevertheless coherent and adaptively functional [2], is an example of problem-solving ability in a high-dimensional latent space. This lynchpin capacity ties together fields of evolutionary developmental biology, non-equilibrium thermodynamics, computational and information science, and the emerging field of diverse intelligence. The implications of understanding the multiscale behavior of the active matter of life during embryogenesis, regeneration, and cancer suppression range across biomedicine, bioengineering, robotics, and bio-inspired artificial intelligence (AI). Central to this set of questions is the relationship between the genetically specified hardware inside cells and the resulting physiological software that produces phenotypes acted upon by selection. Given the plasticity and context-sensitive decision-making of the all-important morphogenetic layer lying between genotype and phenotype, what are useful conceptual frameworks for understanding what genomes actually do (or encode), on evolutionary and ontogenic timescales?

A new perspective: the generative genome

Recent work integrating developmental biology and computer science has provided a new model of how genetic information is encoded and decoded during evolution and embryogenesis. Concepts from biology, neuroscience, and AI now offer a formalizable and timely framework that could advance both theoretical and empirical research [3-5]. Here, we provide a brief overview of related ideas and ways to extend this approach in theoretical and experimental biology.

Evolution through developmental reproduction involves encoding the features and functionality necessary for high-fidelity reconstruction of an organism out of the compact form of the genome. The genome does not encode organismal traits or developmental processes directly (it encodes the protein-level hardware necessary for cell behavior and information processing). The new ideas being developed try to go beyond 'blueprint' or 'program' metaphors and instead identify the genome as compressed latent variables – analogous to Waddington's gene landscape and biologically implemented via protein-encoding sequences and gene regulatory factors – that instantiate organismal development literally as a generative model [3]. It is hypothesized that the genome comprises compressed latent variables that are shaped or encoded by evolution and natural selection, and decoded by a generative model implemented by the cells of the developing embryo that is strikingly similar to the way information is processed in biological and engineered cognitive systems [3,6]. Development can thus be interpreted as a hierarchical generative decoding process from a single cell into a mature organism that is similar but not identical to its ancestors, a reconstruction with variational adaptations and mutations, but also dynamic and flexible interpretation of past information as suitable for new contexts [6]. As we argue later, this kind of architecture enables not only fidelity (reliability) in reaching the correct species-specific target morphology, but also creative problem-solving that maximizes adaptive saliency to new scenarios, not just hardwired replication of what was successful with prior environments and genetics. The line of thinking described herein integrates across scales of space and time, identifying this flexible, creative process as conserved across evolutionary, developmental, and behavioral contexts.

[PAGE 2] Evolutionary development as generative model

In ML terms, such an architecture is reminiscent of variational autoencoders (VAEs) [7-10], two-stage artificial neural networks (ANNs) [11-15] consisting of an encoder and a decoder part (Figure 1A). The encoder and decoder are jointly trained to compress input data into a lower-dimensional bottleneck representation, from which the original data is reconstructed via decompression. Typically, the decoding-stack of VAEs occurs hierarchically, going from abstract representations through adding modular features to detailed reconstructions of the original data, closely resembling, in turn, the developmental stack of embryogenesis. The latent space can capture emergent properties of the data, such as clusters or manifolds that represent different classes or features (modularity). Generative models, including VAEs, generate data by sampling from this latent space, variational latent variables allowing for the creation of novel examples conforming but not identical to the training data (adaptation). Thus, generative processes relying on compressed latent variables arguably leverage modularity and evolvability in developmental biology. Such work speaks to the fundamental question of the meaning of 'genetics' - what precisely is encoded in the biochemical medium of the genes and how is it read out and interpreted by the active cellular material whose hardware it specifies?

Extending the autoencoder analogy

This new framework focuses on the powerful idea of interpreting the genome as latent variables of a generative model. The assertion that DNA instantiates a generative model of the organism [3] has been actively debated [4-6,16-19]. However, authors differ as to whether their proposed generative model covers: (i) the entire developmental stack from the genome to the organism, or targets (ii) a single decoding layer between the genome and a cellular phenotype. This raises questions about how such a model might be applied across different scales of biological organization (cf. Figure 1B):

(i) Biology is organized in layers within layers of abstraction, where the components of each organizational level efficiently navigate their respective problem domains [5,16,17], including metabolic, transcriptomic, physiological, anatomical, and behavioral state spaces. More than that: biological agents, even those comprising the same organism continuously influence (i.e., 'hack') each other either in symbiotic or adversarial relations, including among the organs of a single organism [20-22].

(ii) Even individual cells dynamically respond to environmental cues but can also reconfigure internally via gene regulatory networks (GRNs), displaying significant structural and operational plasticity, including several different kinds of learning [4,5,18,23].

VAEs are single-shot generators with limited generative variability stemming from noise applied at the latent space level. The subsequent decoder is a deterministic downstream process with no room for further variability or creativity. By contrast, organismal development is inherently distributed with a collective of agents constantly reinterpreting and reacting to signals and noise across scales [5,18]. Following the VAE metaphor, this is fundamentally different from variations only in the latent space, and would rather affect every node within the decoder, massively departing from the typical deterministic downstream decoding processes. Moreover, embryonic viability requires adherence to stringent physiological and energetic criteria throughout development, while vanilla VAEs do not enforce physical constraints on intermediate states during decoding.

The hierarchical and recurrent complexity and multiplicity of the genetically instantiated generative model of organismal development poses significant challenges for current ML architectures, greatly departing from the VAE analogy. Here, we intend to expand on the more general generative model framework and discuss inherently collective self-regulatory ML approaches bridging the gap between artificial and biological life.

[PAGE 4] From bowties to Darwin's agential material

One exciting aspect of using deep concepts from ML to shed light on biology is that it provides a rigorous formalism for unifying dynamics of cognitive and morphological change. The idea that the self-assembly of the body had important symmetries with the construction of minds must have already been apparent to Turing, who famously was not only interested in unconventional embodiments of intelligence and patterns of mental activity [24] but also in the self-assembly of structural patterns during development [25]. Another notable contribution to this field was Grossberg's prescient 'Communication, memory, and development' [26]. However, advances in connectionist approaches to problem-solving in synthetic media have provided more specific architectural principles around which somatic and cognitive competency can be organized. [27-32]

Starting in 2016, Pezzulo and Levin [4] discussed how top-down approaches, such as the free energy principle (FEP) [33-36], can model developmental processes through nested bowtie architectures (i.e., where diverse molecular mechanisms converge on critical intermediates to achieve high-level goals through distributed self-orchestration). This concept highlights the efficiency and robustness of the generative developmental and evolutionary process of biological systems. Subsequent works [16,17] further explore scale-free and hierarchical dynamics in development, emphasizing the FEP's unifying role in evolution and development and introducing the concept of biology as a multiscale competency architecture (MCA), with causal top-down and bottom-up control pathways. They specifically proposed that tissue, organs, organ systems, and other levels of biological organization arise as the generalization of compressed information at molecular levels, as occurs in multilayer ANNs, and framed regeneration as a kind of pattern completion task [37] (see Box 1 for explanations of computer science terms).

This led to the idea that each individual within an MCA is an autonomous agent, actively navigating their respective environments and problem spaces [17]. In turn, a unified approach to understanding biological self-organization is arguably rooted in localized homeostatic agents following cooperatively coordinated error correction principles (Figure 1C).

Although somatic cells have been domesticated by multicellularity, and their behavior is regulated by many exogenous factors in vivo to align them toward large-scale setpoints at the organ and tissue level, they navigate physiological, transcriptional, metabolic, and anatomical problem spaces under the influence of their own priors as well as of cues from the local and remote internal environment [18]. Thus, a critical missing piece in our understanding of development and evolution is the fact evolution operates on an agential material, not passive, or even merely active, matter: cells can integrate multiple modalities of sensory data and make context-sensitive decisions [38-41], and even the molecular networks inside of cells can implement several different kinds of learning, including Pavlovian conditioning [23,42,43].

[PAGE 7] Generative bowties everywhere

Mitchell and Cheney [3] identify the genome as latent variables of a generative model for organismal development that is facilitated through GRNs and shaped by evolution. Their proposed VAE architecture leverages developmental plasticity through complexity of the connectionist decoder stack, and evolvability through variability in latent variables. By contrast and complement, our framework [4-6,16-19] emphasizes the fundamental role of goal-directed problem-solving and collective intelligence of biological systems across scales in developmental and evolutionary pathways.

The paradox of change is a profound invariant across spatio-temporal scales, materials, and problem spaces, equally affecting embryos, behaving individuals, and evolutionary lineages. If a system stays the same, it will not survive when circumstances call for novel form or function. But if it changes and adapts, it is no longer the same system and the original has likewise, to an extent, disappeared. Thus, the question of what it is that persists through evolutionary change, morphogenesis/metamorphosis, and learning can be better visualized as a process of contextual sense-making, not an object. We have visualized it as a bowtie dynamic, which is traversed by active agents continuously (Figure 1).

In the case of learning, the center of the bowtie is the 'now' moment. The left side of the bowtie is the deductive, algorithmic process of learning and generalization, which their cognitive system compressed from individual instances of experience. But beings never have access to the actual past - what they have access to is the engrams stored in their brain (and possibly body [51]). Thus, the right side of the bowtie is a creative process in which the cognitive system must reconstruct the meaning of these engrams, but optimizing for saliency in the current situation, not constancy of the meaning these biophysical messages had for their past self.

The same dynamic occurs in embryonic development and evolution. Counter to the familiar model in which the genetics reproduces a map of a fixed body structure, morphogenesis is a creative, problem-solving process which utilizes information in the genome but is not a hardwired, mechanical result of it. The experience of past members of a lineage is stored in a generative seed - the genome, which does not directly code for phenotypic features. The process of producing a functional body given the prompt of the genome is as much a sense-making effort, executed by the biomechanical, biochemical, and bioelectrical networks of protoplasm to navigate physiological and anatomical spaces as is the memory-interpretation task of neural networks in brains as an animal navigates the 3D space of behavior. We propose that what evolution builds are not fixed solutions to fixed environments, but problem-solving agents, exerting well-demonstrated competencies to use genetically provided affordances in new ways as needed (discussed in detail in [5,6]).


[PAGE 8] We have proposed that biology strongly leans in to the fact that it is operating on an unreliable substrate (guaranteed to change due to mutation over long timescales and noise on short timescales) and favors architectures that take the lessons of the past seriously, but not literally, and exhibit considerable regulative and creative ways to navigate their problem spaces. Viewing biology as operating over an active, computational, decision-making material (as opposed to fixed mappings between genetic information and phenotypic outcomes) has many implications for how we relate to it in biomedical settings (recent emphasis being on prompts and behavior-shaping communication [52]), but also for evolution. Recent computational modeling efforts showed that the competency of the material hides information from selection, leading much of the work to be done on the competency mechanisms themselves [48,49]. This results in augmented ability of the material to creatively adapt to changes in its genetics, thus accelerating the evolutionary process, and kickstarting a positive feedback loop which scales primitive competencies of living matter into the more obvious intelligence of brainy animals.

It is essential to develop a rigorous understanding of the substrate- and scale-invariant learning and creative problem-solving dynamics that tie together morphogenesis, evolution, and learning [28,29,53,54].

While the VAE architecture in particular might not yet capture such recurrent multiscale dynamics, fundamental questions remain about the differences and similarities between active inference systems, collective goal-directed agents, and (self-regulatory) generative models, especially in distributed and hierarchical systems. One might interpret every biological agent as a generative model - with an informational bowtie at its core - sampling or generating actions from a policy subordinate to a higher-level goal state. Consequently, organismal development would be characterized as a highly noisy, recurrent, and hierarchically multiplexed generative process.

[PAGE 8] Parallels between construction of body and mind

What we have lived is nothing, what we live is a point, what we have yet to live is not yet a point, but it can be a point, which together will be and have been.
[- Giordano Bruno]

The idea of seeing development as a distributed agential process can be expanded to memory and cognition [6]: a developing 'Self', capable of learning through subjective cognitive experience, is ultimately in charge of reinterpreting its own thoughts, constantly reconstructing its story without having objective ground truth of the semantics of its memories [55]. By interpreting memory as compressed embedding, remembering is an active decoding process of information captured in 'creative' bowties [6]. The process of remembering parallels a context-sensitive generative process akin to associative-memory-based attention mechanisms [56-58] or, in turn, development, rendering 'memories' to be potentially hierarchical and contextually agential, constantly reinterpreted by the same 'Self'.

[PAGE 9] The concept of an MCA is applicable across biological systems, including development, evolution, and cognitive processes: it's agents all the way up and down. In turn, this suggests a unified approach to understanding biological organization – from cells to mind – can not be explained via complexity and (nested) indirect encodings alone, but rather fundamentally rooted in collective intelligence [19].

[PAGE 9] Neural cellular automata for self-orchestrated morphogenesis

Dating back to von Neumann's self-replicating machines [59], cellular automata (CAs) have provided a foundational framework for modeling distributed biological phenomena and Artificial Life, such as replication, growth, and morphogenesis, simply through local interactions of neighboring cells on a discrete grid. Exemplified by, for example, Conway's Game of Life [60] and Wolfram's New Kind of Science [61], even basic update rules amongst a CA's cells can give rise to emergent system-level behavior recapitulating some aspects of the dynamic, constructive quality of life. Since explicitly programming CAs for target applications proved difficult, a relatively recent but promising extension termed neural cellular automata (NCAs) [62,63] replaces the hard-wired update rules in CAs with more flexible and generalizable ANNs. Like the bowtie continuous sense-making of life mentioned earlier, an NCA's cells make dynamic decisions about how to interpret the current state of their neighborhood in order to generate their next state. Such NCAs have been successfully trained via differentiable [63-69] and evolutionary learning frameworks [49,70] to perform, amongst others, the inverse task of targeted pattern formation (cf. Figure 1D).


Just like CAs, the artificial cells (A-cells) of NCAs maintain numerical cell states, analogous to differentiation of biological cells (B-cells). The A-cell's internal ANN computes how to update the cell's state based on the states of its neighbors, modeling decision-making competencies of B-cells. Mirroring the structural and functional plasticity of biological development, a single A-cell can instantiate the growth of a cybernetic tissue via recurrent and fully decentralized intercellular communication and internal computation. This leads to the formation of diverse tissue types, composite organs, etc., by this multicellular substrate until the system-level goal-morphology is achieved. Thus, the functionality of such A-cells can be evolved to "grow” diverse high-fidelity target patterns in-silico, operating as a generative morphogenetic model.


NCAs describe an iterative multiscale generative process capable of solving the inverse pattern formation task of embryogenesis fundamentally based on distributed, multi-agential communication protocols and local iterative error-correction principles [16]. This not only represents a generative morphogenetic process, but also parallels the dynamical self-regulatory architecture of GRNs: an A-cell's state broadly captures the physiological state of B-cells, such as gene expressions or transcription factors, while its ANN substitutes the GRN. The operational point of both networks, ANNs and GRNs, is susceptible to noise, environmental cues, cell state expressions, and memory. In this context, the NCA's ANN can be interpreted as a generative genome representing the latent variables of an evolvable multicellular substrate - an informational bowtie compressed into a single A-cell - while cell states may carry epigenetic information.


NCAs have been successfully trained to perform robust embryogenesis [49,63,64,71,72], symbiotic or parasitic behavior [66,67], and even intricate self-orchestrated computational tasks such as collective classification [69], medical imaging [73,74], or pathfinding in a maze [75]. Moreover, the cellular competencies of morphogenetic NCAs have significant implications for an underlying evolutionary process [49], facilitating enhanced evolvability and transferability to novel problems, and potentially enabling open-ended evolution through concepts from the paradox of robustness [76,77]. Moreover, the A-cells' ANNs may exhibit graph-like recurrent architectures [49], and thus potentially model active inference agents with an internal world-model, whereas cell states represent communication boundaries or Markov blankets. Thus, NCAs are promising models for understanding the MCA of biology and exploiting it in novel ways for top-down control in biomedical [78] and bioengineering [79,80] settings.


[PAGE 10] A model for distributed decision-making

Current research aims to explicitly extend NCAs towards hierarchical architectures [81-83]: departing from locally communicating cellular neighbors on a single NCA layer, hierarchical NCAs capture emerging inter-layer structures (such as clusters of cells) as effective neighbors in successively higher layers of abstraction, thereby allowing intermodular coordination across scales. While potentially improving NCAs' applicability, this might not be necessary conceptually to achieve multiscale dynamics: at the critical point, the Ising model - the simplest CA-like model for a magnetic material - displays self-similar organization patterns of magnetic islands. An NCA operating near such a critical point may exhibit scale-free spatio-temporal activation patterns in its distribution of cell states [84], potentially leading to collective interactions of such higher-level structures. Mirroring the near-critical activation patterns observed in neuronal tissue [85-87], this yet again blurs the line between morphogenesis and behavioral information processing, which is essential for capturing the nature of living things that process information while actively remodeling the processing medium itself (see Box 1 for explanations of physics-related and computer science terms, also for the following).


While indirect encoding paradigms in the domain of reinforcement learning (RL) [88] demonstrate impressive performance [89-93], encoding innate behavior in a genomic bottleneck [94], NCAs have been utilized to facilitate both morphogenetic development and robust, decentralized, and transferable behavioral control in composite RL agents [71,95-97]: purely based on intercellular communication and intracellular information processing, NCAs can grow functional robotic morphologies (by self-assembling synthetic sensory, messenger, and motor cells) that display behavioral problem-solving capabilities at the system-level based on decentralized coordination of their components. Similarly, neural developmental programs have been used to grow an agent's controller ANN during its lifetime [98,99]. Other promising RL approaches [100-102] maintain distributed sensors whose individual recurrent perceptions are analyzed for importance by attention-based mechanisms to retain a compressed environmental representation for subsequent decision-making. Such informational bowties, be it in the single-agent or collective intelligence domain [103], or even in world-model architectures [104-106], allow correspondingly trained – or evolved - agents to generalize well to environmental rules and navigate their respective problem spaces robustly.

[PAGE 10] Diffusion models: development and evolution as incremental denoising

Although progress has been made [70], NCAs are still limited in their capacity of generating multiple patterns from different seed cells. However, NCAs are closely related to probabilistic diffusion models (DMs) [107-111], cutting-edge generative techniques that demonstrate versatility and control over - so far - mainly multimedia content generation.

Both methods iteratively refine an originally incomplete or noisy data sample through incremental error correction until it conforms to a target data distribution. Different training and denoising paradigms based on non-equilibrium thermodynamics [111,112] allow DMs to generate realistic samples that capture versatile data distributions, a process that can even be guided by text prompts [113,114]. To name but a few examples, DMs have revolutionized guided generation of synthetic images [113], videos [115], and protein-folding predictions [116], and even facilitate fully generative game-play [117] demonstrating agential behavior.

Notably, we have recently demonstrated [118,119] that DMs can serve as highly efficient generative models in evolutionary algorithms, substituting conditional genotypic offspring generation through iterative denoising of genomic parametrizations in various optimization domains. Moreover, the generative process of DMs is capable of adapting a given data instance conditionally to some external cues or constraints, such as modularly replacing details in sceneries [120], transforming an image to a particular target style without corrupting its content [121], or adjusting an RL agent's policy dynamically to achieve targeted behavior shaping [119]. Thus, DMs (and similarly NCAs) are promising candidates to model the modular, internal and external context-sensitive reconfiguration capabilities of B-cells.


While the flow of information in the VAE model of evolutionary development is deterministic and unidirectional when traversing a hard-wired and predefined decoder stack - after possible variations in the latent space – both DMs and NCAs iteratively operate on dynamically refined structures. More than that, these increasingly contextualized structures are recursively re-introduced as refined inputs to successive generative steps, effectively conditioning and reconfiguring the functionality of their governing generative models in a dynamic, context-sensitive manner. Through a series of phase transitions from high entropy noise to novel structured samples, DMs organize data hierarchically [122-125]: initial features, emerging at early stages during the generative phase, are later refined via complexification. Details are iteratively added to initially rough sketches of the target pattern, while the order of feature refinement is modularly interchangeable [122]. Instead of a downstream latent space encoding and decoding, DMs capture the latent variables of a self-regulatory generative process, generalizing the dynamics of blending hierarchical (phenotypical) features contextually. This mirrors biological development much more accurately, where the output of computational processes is constantly fed back to the biological hardware, inducing architectural and topological changes to itself, and driving the morphological and functional plasticity of this process.


Given their generative fidelity and context-sensitive versatility, DMs potentially represent powerful models for morphogenesis and, in the spirit of this contribution, might be candidates for foundational generative models for DNA or GRNs with deep associative memory capacities [124,126]: the DM would literally represent the generative and functional genome [3,4], while the data sample it operates on relates to gene expressions or transcriptomic data [119].

[PAGE 12] Concluding remarks

The material of life exhibits active problem-solving capacities in physiological and anatomical spaces which dwarf our current efforts in biomedicine and bioengineering, in the same way that brains' learning capacities are still unmatched by efforts in ML and AI (Figures 2 and 3). An especially impressive example are asexual strains of planarian flatworms, which overcome whole body damage, cancer, and aging (despite a very noisy genome, which accumulates mutations due to somatic inheritance [127]), can implement other species' morphotypes [128] and novel, permanent bodyplan architectures in the absence of genetic change [129,130] (Figures 2D,E and 3A), and identify a small number of genes sufficient to counteract the effects of a novel toxin [131]. The challenges and opportunities facing the field (see Outstanding questions) now is to exploit and extend the deep lessons of connectionist neuroscience and ML, with respect to architectures that learn and adapt. Exciting developments in genetics include fundamental changes in how we see the subject of the science itself, and what we are really doing when we track and edit genetic information. The molecular medicine of the future, and our understanding of natural and experimentally guided evolution, depends on developing omics tools, modeling approaches, and interventions that extend classic views of the mapping between genetics and phenotypes, and tames system-level complex outcomes by targeting the genetic, physiological, and informational structures as active learning agents.